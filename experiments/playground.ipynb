{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbias_bench\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[1;32m      4\u001b[0m reload(models)\n",
      "File \u001b[0;32m~/LLM-bias/bias-bench/bias_bench/model/models.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/__init__.py:1264\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizable\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantized\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqat\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintrinsic\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m _C\u001b[38;5;241m.\u001b[39m_init_names(\u001b[38;5;28mlist\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_storage_classes))\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/qat/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"QAT Dynamic Modules\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mThis package is in the process of being deprecated.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mPlease, use `torch.ao.nn.qat.dynamic` instead.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modules  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/qat/dynamic/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"QAT Dynamic Modules\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mThis package is in the process of being deprecated.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mPlease, use `torch.ao.nn.qat.dynamic` instead.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/qat/dynamic/modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/qat/dynamic/modules/linear.py:10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"QAT Modules\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mThis file is in the process of migration to `torch/ao/nn/qat/dynamic`, and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mwhile adding an import statement here.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/ao/nn/qat/dynamic/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/ao/nn/qat/dynamic/modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bias_bench.model import models\n",
    "\n",
    "from importlib import reload\n",
    "reload(models)\n",
    "\n",
    "# model_name =  \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "generator = models.Private_Generator(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bias_bench.model import models\n",
    "from importlib import reload\n",
    "reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yandan/miniconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/yandan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name_1 \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-70B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model_name_1 = \"meta-llama/Meta-Llama-3-8B\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m generator_1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPrivate_Generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM-bias/bias-bench/bias_bench/model/models.py:87\u001b[0m, in \u001b[0;36mPrivate_Generator.__init__\u001b[0;34m(self, model_name_or_path)\u001b[0m\n\u001b[1;32m     79\u001b[0m     quant_config \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mBitsAndBytesConfig( \u001b[38;5;66;03m# NF4 quantization\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     81\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m         bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Load in 4 bits\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/yandan/transformers_cache\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Automatically distribute the model across available GPUs\u001b[39;49;00m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquant_config\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is loaded on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name_or_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/modeling_utils.py:3963\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3960\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3962\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3963\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3966\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:102\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from bias_bench.model import models\n",
    "\n",
    "model_name_1 =  \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "# model_name_1 = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "generator_1 = models.Private_Generator(model_name_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "stereoset_path = \"../data/stereoset/test.json\"\n",
    "\n",
    "with open(stereoset_path, 'r') as file:\n",
    "    stereoset_data = json.load(file)\n",
    "\n",
    "stereoset_data = stereoset_data['data'][\"intersentence\"]\n",
    "# if args.percentage != 100:\n",
    "# sample_size = int(len(stereoset_data) * (1 / 100))\n",
    "# stereoset_data = random.sample(stereoset_data, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = int(len(stereoset_data) * (float(20) / 100))\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "dump_stereoset_path = \"../data/stereoset/adapt_part\"\n",
    "if not os.path.exists(dump_stereoset_path):\n",
    "    os.makedirs(dump_stereoset_path)\n",
    "\n",
    "for i in range(5):\n",
    "    dump_data = []\n",
    "    dump_data = stereoset_data[0+(i*sample_size):sample_size+(i*sample_size)]  \n",
    "    if i == 4:\n",
    "        dump_data = stereoset_data[0+(i*sample_size):] \n",
    "    # print(dump_data)\n",
    "    with open(f'{dump_stereoset_path}/test_part_{(i+1)}.json', 'w') as json_file:\n",
    "        # Dump the data to the file\n",
    "        json.dump(dump_data, json_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict \n",
    "from bias_bench.adaptation.prompt_context import get_stereo_context\n",
    "import re\n",
    "\n",
    "\n",
    "def get_inter_stereo_prompt(instances) -> List[str]:\n",
    "    model_prompts = list()\n",
    "    for instance in instances:\n",
    "        bias_type = instance['bias_type']\n",
    "        target = instance['target']\n",
    "        model_prompts += [f\"\"\"\n",
    "\n",
    "            Task: Adapt the StereoSet Bias dataset from its USA context to a Singaporean context.\n",
    "            Objectives: Adapt wording to fit the Singaporean context (race, gender, religion, profession). Preserve dataset structure and labels. Ensure cultural relevance for Singapore's multicultural society.\n",
    "            \n",
    "            Instance Details: target: \n",
    "            Modify to a relevant group in Singapore if possible.\n",
    "            bias_type: Keep strictly the same.\n",
    "            Context: Adjust to reflect Singaporean situations.\n",
    "            Sentences & Labels: Rewrite sentences (Sentence_1, Sentence_2, and Sentence_3) to fit the Singaporean context, maintaining correct labels (stereotype, anti-stereotype, unrelated), assign each label to each sentence once.\n",
    "            If Adaptation is Difficult: Leave unchanged and state \"Reason: [reason]\" or \"Reason: NA\".\n",
    "            \n",
    "            Singapore Bias Profile: {get_stereo_context(bias_type)}\n",
    "\n",
    "            Format your response as a JSON object with \"Adapted\":\n",
    "            Target: {target}\n",
    "            bias_type: {bias_type}\n",
    "            Context: {instance['context']}\n",
    "            Sentence_1: {instance['sentences'][0]['sentence']}\n",
    "            Label_1: {instance['sentences'][0]['gold_label']}\n",
    "            Sentence_2: {instance['sentences'][1]['sentence']}\n",
    "            Label_2: {instance['sentences'][1]['gold_label']}\n",
    "            Sentence_3: {instance['sentences'][2]['sentence']}\n",
    "            Label_3: {instance['sentences'][2]['gold_label']}\n",
    "            Adapted: JSON\"\"\"\n",
    "        ]\n",
    "        \n",
    "\n",
    "    return model_prompts\n",
    "            # {{\\\"explanation\\\": \\\"<brief_explanation_less_than_50_words>\\\", \\\"answer\\\": \\\"<answer_letter>\\\"}}\\nContext: {instance['context']}\\nA) {instance['sentences']['sentence'][0]}\\nB) {instance['sentences']['sentence'][1]}\\nC) {instance['sentences']['sentence'][2]}\\n\\nAnswer JSON:\"]\n",
    "            # {{\\\"target\\\": \"[Adapted Target]\",\\n \\\"bias_type\\\": \"[Bias Type]\",\\n \"context\": \"[Adapted Context]\", \"sentences\": [{\"sentence\": \"[Adapted Sentence 1]\", \"label\": \"[Label 1]\"}, {\"sentence\": \"[Adapted Sentence 2]\", \"label\": \"[Label 2]\"}, {\"sentence\": \"[Adapted Sentence 3]\", \"label\": \"[Label 3]\"}], \"reason\": \"[Your reason or 'NA']\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict \n",
    "from bias_bench.adaptation.prompt_context import get_stereo_context\n",
    "import re\n",
    "\n",
    "\n",
    "def get_inter_stereo_prompt(instances) -> List[str]:\n",
    "    model_prompts = list()\n",
    "    for instance in instances:\n",
    "        bias_type = instance['bias_type']\n",
    "        target = instance['target']\n",
    "        model_prompts += [f\"\"\"Please adapt the StereoSet Bias dataset from its Western-oriented context to a Singaporean context. Objectives: - Adapt wording to align with the Singaporean context, focusing on **race, gender, religion, and profession**. **Adaptation Guidelines**: - target: Modify to a relevant group in Singapore if applicable; this field is optional for adaptation. - bias_type: Keep unchanged. - context: Adjust to reflect situations common in Singapore. - sentences and labels: Rewrite each sentence to align with the Singaporean context, preserving its intended bias category (stereotype, anti-stereotype, unrelated). Ensure each label is correctly assigned, and each bias type is used only once. **Handling Difficult Adaptations**: - If adaptation is challenging, leave the instance unchanged and provide a reason (maximum 50 words) in the format: `\"reason: [your reason]\"`. If you helped adapt the sentences, state `\"reason: NA\"`. **Singapore Bias Profile**: - {{get_stereo_context{bias_type}}} **Adaptation Instructions**: Given the following JSON object from the StereoSet dataset, provide the adapted version in the same format, ensuring each sentence has its label correctly assigned and including the `\"reason\"` field: {{ \"target\": \"{target}\", \"bias_type\": \"{bias_type}\", \"context\": \"{instance['context']}\", \"sentences\": [ {{\"sentence\": \"{instance['sentences'][0]['sentence']}\", \"label\": \"{instance['sentences'][0]['gold_label']}\"}}, {{\"sentence\": \"{instance['sentences'][1]['sentence']}\", \"label\": \"{instance['sentences'][1]['gold_label']}\"}}, {{\"sentence\": \"{instance['sentences'][2]['sentence']}\", \"label\": \"{instance['sentences'][2]['gold_label']}\"}} ], \"reason\": \"[Your reason or 'NA']\"}} Provide the adapted version in the above JSON format after 'Your answer: '. Your answer: JSON\"\"\".strip()]\n",
    "\n",
    "    return model_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bias_bench.adaptation import prompt_strings\n",
    "from importlib import reload\n",
    "reload(prompt_strings)\n",
    "\n",
    "batch_prompts = get_inter_stereo_prompt(stereoset_data)\n",
    "\n",
    "i = 4\n",
    "\n",
    "prompt = batch_prompts[i]\n",
    "original_set = stereoset_data[i]\n",
    "batch_response = generator_1.response_generator(prompt)\n",
    "# key_result = prompt_strings.extract_sample_from_response(\"stereoset\", batch_response[0])\n",
    "print(batch_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_json = re.sub(f'^.*?{re.escape(\"{\")}', '{', batch_response[0])\n",
    "result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.loads(result_json)['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stereoset_data[1]['context'])\n",
    "print(stereoset_data[1]['sentences'])\n",
    "print(stereoset_data[1]['bias_type'])\n",
    "print(stereoset_data[1]['target'])\n",
    "print(stereoset_data[1]['sentences'][0]['sentence'])\n",
    "print(stereoset_data[1]['sentences'][0]['gold_label'])\n",
    "print(stereoset_data[1]['sentences'][1]['sentence'])\n",
    "print(stereoset_data[1]['sentences'][1]['gold_label'])\n",
    "print(stereoset_data[1]['sentences'][2]['sentence'])\n",
    "print(stereoset_data[1]['sentences'][2]['gold_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance in stereoset_data['data']['intersentence']:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bias_bench.model import models\n",
    "from bias_bench.adaptation import prompt_strings\n",
    "\n",
    "batch_prompts = prompt_strings.get_intra_stereo_prompt(stereoset_data)\n",
    "batch_response = generator.response_generator()\n",
    "key_result = prompt_strings.extract_sample_from_response(\"stereoset\", batch_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dump_data = {\"version\": \"1.0-test-sg-adapted\", \"data\": {\"intersentence\": \"\"}} \n",
    "\n",
    "with open(\"../data/stereoset/adapted/adapted_full_intersentence_test.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "dump_data[\"data\"][\"intersentence\"] = data\n",
    "\n",
    "with open(\"../data/stereoset/adapted/test_intersentence_sg.json\", 'w', encoding='utf8') as dump_file:\n",
    "    json.dump(dump_data, dump_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"../data/stereoset/adapted/test_intersentence_sg.json\", 'r') as file:\n",
    "\n",
    "    inter = json.load(file)[\"data\"][\"intersentence\"]\n",
    "\n",
    "with open(\"../data/stereoset/test.json\") as o_file:\n",
    "    original = json.load(o_file)[\"data\"][\"intersentence\"]\n",
    "\n",
    "items = {\"item_id\":\"\",\n",
    "         \"target_adapted\":\"\", \"target_original\": \"\",\n",
    "         \"context_adapted\":\"\",\"context_original\":\"\",\n",
    "         \"bias_adapted\":\"\",\"bias_original\":\"\",\n",
    "         \"sentence_1_adapted\":\"\",\"sentence_1_original\":\"\",\n",
    "         \"sentence_1_label_adapted\":\"\",\"sentence_1_label_original\":\"\",\n",
    "         \"sentence_2_adapted\":\"\",\"sentence_2_original\":\"\",\n",
    "         \"sentence_2_label_adapted\":\"\", \"sentence_2_label_original\":\"\",\n",
    "         \"sentence_3_adapted\":\"\",\"sentence_3_original\":\"\",\n",
    "         \"sentence_3_label_adapted\":\"\",\"sentence_3_label_original\":\"\",\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([items])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>target_adapted</th>\n",
       "      <th>target_original</th>\n",
       "      <th>context_adapted</th>\n",
       "      <th>context_original</th>\n",
       "      <th>bias_adapted</th>\n",
       "      <th>bias_original</th>\n",
       "      <th>sentence_1_adapted</th>\n",
       "      <th>sentence_1_original</th>\n",
       "      <th>sentence_1_label_adapted</th>\n",
       "      <th>sentence_1_label_original</th>\n",
       "      <th>sentence_2_adapted</th>\n",
       "      <th>sentence_2_original</th>\n",
       "      <th>sentence_2_label_adapted</th>\n",
       "      <th>sentence_2_label_original</th>\n",
       "      <th>sentence_3_adapted</th>\n",
       "      <th>sentence_3_original</th>\n",
       "      <th>sentence_3_label_adapted</th>\n",
       "      <th>sentence_3_label_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  item_id target_adapted target_original context_adapted context_original  \\\n",
       "0                                                                           \n",
       "\n",
       "  bias_adapted bias_original sentence_1_adapted sentence_1_original  \\\n",
       "0                                                                     \n",
       "\n",
       "  sentence_1_label_adapted sentence_1_label_original sentence_2_adapted  \\\n",
       "0                                                                         \n",
       "\n",
       "  sentence_2_original sentence_2_label_adapted sentence_2_label_original  \\\n",
       "0                                                                          \n",
       "\n",
       "  sentence_3_adapted sentence_3_original sentence_3_label_adapted  \\\n",
       "0                                                                   \n",
       "\n",
       "  sentence_3_label_original  \n",
       "0                            "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance in inter:\n",
    "    id_adapt = instance[\"id\"]\n",
    "    for o_instance in original:\n",
    "        id_original = o_instance[\"id\"]\n",
    "        if id_original == id_adapt:\n",
    "            assert o_instance[\"bias_type\"] == instance[\"bias_type\"]\n",
    "\n",
    "            items[\"item_id\"] = id_original\n",
    "\n",
    "            items[\"target_original\"] = o_instance['target']\n",
    "            items[\"target_adapted\"] = instance[\"target\"]\n",
    "\n",
    "            items['bias_adapted'] = instance[\"bias_type\"]\n",
    "            items['bias_original'] = o_instance['bias_type']\n",
    "\n",
    "            if (o_instance['context'] == instance['context'] &\n",
    "                 o_instance['sentences'][0]['sentence'] == instance['context'][0]['sentence'] &\n",
    "                 o_instance['sentences'][1]['sentence'] == instance['context'][1]['sentence'] &\n",
    "                 o_instance['sentences'][2]['sentence'] == instance['context'][2]['sentence']):\n",
    "\n",
    "\n",
    "\n",
    "            items[\"sentence_1_adapted\"] = \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
